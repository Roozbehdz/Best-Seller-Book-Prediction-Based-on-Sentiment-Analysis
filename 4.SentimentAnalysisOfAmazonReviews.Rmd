---
title: 'Sentiment Analysis of Amazon Reviews '
author: "Roozbeh"
date: "8/27/2019"
output:
  word_document: default
  html_document: default
---

```{r libraries}
# libraries
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidytext)
library(textdata)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(reshape2)

```



```{r getting the data and tokenizing it by words}
# getting the data and tokenizing it by words
data = read.csv("C:/Users/10/Desktop/GoodReads_TextMining-master/books/The Time Traveler's Wife.csv")

data_tidy = data[]
data_tidy$date = format(as.Date(data_tidy$date, "%d-%b-%y"))
data_tidy$comments = as.character(data_tidy$comments)
data_tidy = as_tibble(data_tidy)
data_tidy = data_tidy %>% arrange(desc(date))
data_tidy_token_word = as_tibble(data_tidy) %>%
  unnest_tokens(word, comments)

```

```{r sentiment analysis based on NRC lexicon}
# sentiment analysis based on NRC lexicon
data_tidy_sentiment_nrc = data_tidy_token_word %>%
  inner_join(lexicon_nrc())

data_tidy_sentiment_nrc %>%
  count(word, sort = TRUE)
  #count(word, sort = TRUE)

data_tidy_sentiment_nrc %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()

nrcjoy <- lexicon_nrc() %>%
filter(sentiment == "joy")

data_tidy_token_word %>%
inner_join(nrcjoy) %>%
count(word, sort = TRUE)


nrcpositive <- lexicon_nrc() %>%
filter(sentiment == "positive")

data_tidy_token_word %>%
inner_join(nrcpositive) %>%
count(word, sort = TRUE)


nrctrust <- lexicon_nrc() %>%
filter(sentiment == "trust")

data_tidy_token_word %>%
inner_join(nrctrust) %>%
count(word, sort = TRUE)
```



```{r sentiment analysis based on BING lexicon}
# sentiment analysis based on BING lexicon
data_tidy_sentiment_bing = data_tidy_token_word %>%
  inner_join(get_sentiments("bing")) %>%
  count(X, date, title, stars, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)



ggplot(data_tidy_sentiment_bing, aes(date, sentiment, fill = title))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~stars,ncol = 2, scales = "free_x")

ggplot(data_tidy_sentiment_bing %>%
  arrange(date), aes(date, sentiment, fill = title))+
  geom_col(show.legend = FALSE)
```

```{r sentiment analysis based on AFINN lexicon}
# sentiment analysis based on AFINN lexicon
data_tidy_sentiment_afinn <- data_tidy_token_word %>%
  inner_join(lexicon_afinn()) %>%
  group_by(title, date) %>%
  summarise(sentiment = sum(value)) %>%
  mutate(method = "AFINN")


```


```{r comparing NRc,BING and AFINN}
# comparing NRc,BING and AFINN
data_tidy_sentiment_bing_and_nrc <- bind_rows(
  data_tidy_token_word %>%
  inner_join(lexicon_bing()) %>%
  mutate(method = "Bing et al."),
  data_tidy_token_word %>%
  inner_join(lexicon_nrc() %>%
  filter(sentiment %in% c("positive",
  "negative"))) %>%
  mutate(method = "NRC")) %>%
  count(method, title, date, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bind_rows(data_tidy_sentiment_afinn,
data_tidy_sentiment_bing_and_nrc) %>%
ggplot(aes(date, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
```


```{r Contribution to sentiment}
# Contribution to sentiment
bing_word_counts <- data_tidy_token_word %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()

bing_word_counts %>%
group_by(sentiment) %>%
top_n(20) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()

custom_stop_words <- bind_rows(data_frame(word = c("fiction","time","travel","book","story"),
lexicon = c("custom")),
stop_words)
```

```{r wordclouds}
# wordclouds
data_tidy_token_word  %>%
anti_join(custom_stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))


data_tidy_token_word %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "blue"),
max.words = 100)
```

```{r tokenizing comments based on sentences}
# tokenizing comments based on sentences
PandP_sentences <- as_tibble(data_tidy) %>%
unnest_tokens(sentence, comments, token = "sentences")
```

```{r}
book_words <- as_tibble(data_tidy) %>%
unnest_tokens(word, comments) %>%
count(title, word, stars,date, sort = TRUE) %>%
ungroup()
total_words <- book_words %>%
group_by(title) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)


ggplot(book_words, aes(n/total, fill = date)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~stars, ncol = 2, scales = "free_y")
```

```{r calculating word frequencies}
# calculating word frequencies
custom_stop_words <- bind_rows(data_frame(word = c("time","clare","henry","book","claire","story","page"),
lexicon = c("custom")),
stop_words)

freq_by_rank <- book_words %>%
anti_join(custom_stop_words) %>%
group_by(title) %>%
mutate(rank = row_number(),
`term frequency` = n/total)

freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = date)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
```


```{r tf-idf}
# tf-idf
book_words <- book_words %>%
bind_tf_idf(word, title, n)

book_words %>%
select(-total) %>%
arrange(desc(tf_idf))
```



```{r}
book_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(stars) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = title)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~stars, ncol = 2, scales = "free") +
coord_flip()
```


```{r tokenizing comments based on two words together}
# tokenizing comments based on two words together
data_tidy_bigrams <- as_tibble(data_tidy) %>%
unnest_tokens(bigram, comments, token = "ngrams", n = 2)

data_tidy_bigrams %>%
count(bigram, sort = TRUE)

bigrams_separated <- data_tidy_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
```


```{r}
as_tibble(data_tidy) %>%
unnest_tokens(trigram, comments, token = "ngrams", n = 3) %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word,
!word3 %in% stop_words$word) %>%
count(word1, word2, word3, sort = TRUE)
```



```{r}
bigrams_filtered %>%
filter(word2 == "book") %>%
count(title, word1, sort = TRUE)
```





